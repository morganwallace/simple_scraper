{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/local/bin/python\n",
      "# use urllib to get the header information for a web page at location url\n",
      "import urllib2\n",
      "# import urllib.parse\n",
      "# import urllib.robotparser\n",
      "from bs4 import BeautifulSoup\n",
      "from time import sleep\n",
      "import re\n",
      "from ast import literal_eval\n",
      "# import pickle\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def RequestResponse(url):\n",
      "    try:\n",
      "        response = urllib2.urlopen(url)\n",
      "        return response\n",
      "    except urllib.error.URLError as err:\n",
      "        print(\"Error opening url {} .\\nError is: {}\".format(url, err))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def Relative_URL_Checker(url,originator_url):\n",
      "    if url[0] == '/':\n",
      "        parsed_url = urlparse(url)\n",
      "        origParsed = urlparse(originator_url)\n",
      "        hostname = \"http://\" + origParsed.netloc\n",
      "        new_full_url = urllib.parse.urljoin(hostname, parsed_url.path)\n",
      "        return new_full_url\n",
      "    else: \n",
      "    \treturn url\n",
      "    \t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape(url,DOMclass=\"entry-content\"):\n",
      "\tprint(\"scraping\")\n",
      "\tresponse = RequestResponse(url)\n",
      "\tif not response: return\n",
      "\tsoup = BeautifulSoup(response.read())\n",
      "\t# title=soup.head.title.get_text() #save title\n",
      "\t# texts=soup.body.findAll(text=True)#save visible webpage text\n",
      "\twods=soup.find_all('div', class_=DOMclass)\n",
      "\treturn wods"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "non-keyword arg after keyword arg (<ipython-input-17-e794974a3fd3>, line 8)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-e794974a3fd3>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    wods=soup.find_all('div', class_=DOMclass,\"p\")\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-keyword arg after keyword arg\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scrape('http://berkeley-crossfit.com/2013/12/03/wod-12-03-2013/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "scraping\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[<div class=\"entry-content\">\n",
        "<p>A.<br/>\n",
        "Back Squat<br/>\n",
        "3-3-3-3</p>\n",
        "<p>B.<br/>\n",
        "21-15-9<br/>\n",
        "Back Squat (155/105)<br/>\n",
        "Pull-Ups</p>\n",
        "<p><a href=\"http://berkeleycrossfit.files.wordpress.com/2013/12/20131202-234315.jpg\"><img alt=\"20131202-234315.jpg\" class=\"alignnone size-full\" src=\"http://berkeleycrossfit.files.wordpress.com/2013/12/20131202-234315.jpg?w=670\"/></a><br/>\n",
        "Welcome to BCF Charlotte!</p>\n",
        "<p><a href=\"http://berkeleycrossfit.files.wordpress.com/2013/12/20131202-234349.jpg\"><img alt=\"20131202-234349.jpg\" class=\"alignnone size-full\" src=\"http://berkeleycrossfit.files.wordpress.com/2013/12/20131202-234349.jpg?w=670\"/></a></p>\n",
        "<div class=\"wpcnt\">\n",
        "<div class=\"wpa\">\n",
        "<a class=\"wpa-about\" href=\"http://wordpress.com/about-these-ads/\" rel=\"nofollow\">About these ads</a>\n",
        "<script type=\"text/javascript\">\n",
        "\t\tvar wpcom_adclk_hovering = false;\n",
        "\t\tvar wpcom_adclk_recorded = false;\n",
        "\t\tvar wpcom_adclk_theme = \"Visual\";\n",
        "\t\tvar wpcom_adclk_slot = \"wpcom_below_post_adsafe\";\n",
        "\t\tvar wpcom_adclk_network = ( typeof wpcom_adclk_network === \"undefined\" ) ? \"\" : wpcom_adclk_network ;\n",
        "\n",
        "\t\tjQuery(document).ready( function() {\n",
        "\t\t\tfunction wpcom_adclk_hover_yes() { wpcom_adclk_hovering = true; }\n",
        "\t\t\tfunction wpcom_adclk_hover_no() { wpcom_adclk_hovering = false; }\n",
        "\t\t\tjQuery(\".wpa\").click(wpcom_adclk_click);\n",
        "\t\t\tjQuery(\".wpa iframe\").hover( wpcom_adclk_hover_yes, wpcom_adclk_hover_no );\n",
        "\t\t\tjQuery(\".wpa object\").hover( wpcom_adclk_hover_yes, wpcom_adclk_hover_no );\n",
        "\n",
        "\t\t\tjQuery(window).blur( function() {\n",
        "\t\t\t\tif ( wpcom_adclk_hovering ) { wpcom_adclk_click(); }\n",
        "\t\t\t});\n",
        "\t\t});\n",
        "\n",
        "\t\tfunction wpcom_adclk_impression() {\n",
        "\t\t\tvar stat_gif = document.location.protocol + \"//pixel.wp.com/g.gif?v=wpcom-no-pv\";\n",
        "\t\t\tstat_gif += \"&x_ads_imp_theme=\" + wpcom_adclk_theme;\n",
        "\t\t\tstat_gif += \"&x_ads_imp_placement=\"+wpcom_adclk_slot;\n",
        "\t\t\tstat_gif += \"&x_ads_imp_network=\" + wpcom_adclk_network;\n",
        "\t\t\tstat_gif += \"&x_ads_imp_theme_network=\"+wpcom_adclk_theme+\"_\"+wpcom_adclk_network;\n",
        "\t\t\tnew Image().src = stat_gif + \"&baba=\" + Math.random();\n",
        "\t\t\treturn true;\n",
        "\t\t}\n",
        "\n",
        "\t\tfunction wpcom_adclk_click() {\n",
        "\t\t\tif (wpcom_adclk_recorded) { return true; } // no double counting\n",
        "\t\t\tvar stat_gif = document.location.protocol + \"//pixel.wp.com/g.gif?v=wpcom-no-pv\";\n",
        "\t\t\tstat_gif += \"&x_ads_click_theme=\" + wpcom_adclk_theme;\n",
        "\t\t\tstat_gif += \"&x_ads_click_placement=\"+wpcom_adclk_slot;\n",
        "\t\t\tstat_gif += \"&x_ads_click_network=\" + wpcom_adclk_network;\n",
        "\t\t\tstat_gif += \"&x_ads_click_theme_network=\"+wpcom_adclk_theme+\"_\"+wpcom_adclk_network;\n",
        "\n",
        "\t\t\tnew Image().src = stat_gif + \"&baba=\" + Math.random();\n",
        "\t\t\twpcom_adclk_recorded = true;\n",
        "\t\t\tvar now=new Date(); var end=now.getTime()+250;\n",
        "\t\t\twhile(true){now=new Date();if(now.getTime()>end){break;}}\n",
        "\t\t\treturn true;\n",
        "\t\t}\n",
        "\t\n",
        "if ( typeof GA_googleAddAttr == 'function' ) {\n",
        "GA_googleAddAttr(\"AdOpt\", \"1\");\n",
        "GA_googleAddAttr(\"Origin\", \"other\");\n",
        "GA_googleAddAttr(\"LangId\", \"1\");\n",
        "GA_googleAddAttr(\"Domain\", \"berkeley-crossfit.com\");\n",
        "GA_googleAddAttr(\"BlogId\", \"28803641\");\n",
        "GA_googleAddAttr(\"PageURL\", \"http://berkeley-crossfit.com/2013/12/03/wod-12-03-2013/\");\n",
        "GA_googleAddAttr(\"AdSafe\", \"1\");\n",
        "GA_googleAddAttr(\"Tag\", \"wod\");\n",
        "GA_googleAddAttr(\"Partner\", \"AOL\");\n",
        "GA_googleAddAttr(\"theme_bg\", \"292929\");\n",
        "GA_googleAddAttr(\"theme_border\", \"cccccc\");\n",
        "GA_googleAddAttr(\"theme_text\", \"eeeeee\");\n",
        "GA_googleAddAttr(\"theme_link\", \"40d7bc\");\n",
        "GA_googleAddAttr(\"theme_url\", \"40d7bc\");\n",
        "GA_googleAddAdSensePageAttr(\"google_page_url\", \"http://berkeley-crossfit.com/2013/12/03/wod-12-03-2013/\");\n",
        "GA_googleFillSlot(\"wpcom_below_post_adsafe\");\n",
        "}\n",
        "</script>\n",
        "</div>\n",
        "</div>\n",
        "<script type=\"text/javascript\">\n",
        "jQuery( window ).load( function() {\n",
        "    if ( jQuery(\".wpa script[src*='virool.com']\").length > 0 || jQuery(\".wpa script[src*='shareth.ru']\").length > 0 || jQuery(\".wpa iframe[src*='boomvideo.tv']\").length > 0 || jQuery(\".wpa iframe[src*='viewablemedia.net']\").length > 0 || jQuery(\".wpa .sharethrough-placement\").length > 0 ) {\n",
        "        jQuery( '.wpa' ).css( 'width', '400px' );\n",
        "    }\n",
        "setTimeout(function(){if(typeof GS_googleAddAdSenseService !== 'function'){new Image().src=document.location.protocol+\"//pixel.wp.com/g.gif?v=wpcom-no-pv&x_noads=adblock&baba=\"+Math.random()}},100);\n",
        "} );\n",
        "</script>\n",
        "<div class=\"sharedaddy sharedaddy-dark sd-like-enabled sd-sharing-enabled\" id=\"jp-post-flair\"><div class=\"sharedaddy sd-sharing-enabled\"><div class=\"robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing\"><h3 class=\"sd-title\">Share this:</h3><div class=\"sd-content\"><ul><li class=\"share-twitter\"><a class=\"share-twitter sd-button share-icon\" href=\"http://berkeley-crossfit.com/2013/12/03/wod-12-03-2013/?share=twitter\" id=\"sharing-twitter-3200\" rel=\"nofollow\" title=\"Click to share on Twitter\"><span>Twitter</span></a></li><li class=\"share-facebook\"><a class=\"share-facebook sd-button share-icon\" href=\"http://berkeley-crossfit.com/2013/12/03/wod-12-03-2013/?share=facebook\" id=\"sharing-facebook-3200\" rel=\"nofollow\" title=\"Share on Facebook\"><span>Facebook</span></a></li><li class=\"share-end\"></li></ul></div></div></div><div class=\"sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded\" data-name=\"like-post-frame-28803641-3200-54321a9118283\" data-src=\"//widgets.wp.com/likes/#blog_id=28803641&amp;post_id=3200&amp;origin=berkeleycrossfit.wordpress.com&amp;obj_id=28803641-3200-54321a9118283\" id=\"like-post-wrapper-28803641-3200-54321a9118283\"><h3 class=\"sd-title\">Like this:</h3><div class=\"likes-widget-placeholder post-likes-widget-placeholder\" style=\"height:55px\"><span class=\"button\"><span>Like</span></span> <span class=\"loading\">Loading...</span></div><span class=\"sd-text-color\"></span><a class=\"sd-link-color\"></a></div>\n",
        "<div class=\"jp-relatedposts\" id=\"jp-relatedposts\">\n",
        "<h3 class=\"jp-relatedposts-headline\"><em>Related</em></h3>\n",
        "</div></div> </div>]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def visible(element):\n",
      "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
      "        return False\n",
      "    elif re.match('<!--.*-->', str(element)):\n",
      "        return False\n",
      "    return True\n",
      "\n",
      "link_queue=[\"http://ischool.berkeley.edu/\"] #initiate the list to store a queue of links for the crawl\n",
      "all_links=[\"http://ischool.berkeley.edu/\"] #keeps track of all links, so not crawled more than once\n",
      "webPageDataDict={}\n",
      "\n",
      "#extract all the links (a tags) from a web page; print first 10\n",
      "def GetLinksFromPage(url,ID):\n",
      "\tresponse = RequestResponse(url)\n",
      "\tif not response: return\n",
      "\tsoup = BeautifulSoup(response.readall())\n",
      "\ttitle=soup.head.title.get_text() #save title\n",
      "\ttexts=soup.body.findAll(text=True)#save visible webpage text\n",
      "\tpageText=\"\".join(x for x in filter(visible,texts)).replace(\"\\n\",\" \") # remove newline characters\n",
      "\twebPageDataDict[ID]={\"ID\":ID,\"Title\":title,\"URL\":url,\"Text\":pageText.encode(\"ascii\",\"ignore\")} #store page data in dictionary value\n",
      "\ta_tags = soup.find_all('a')\n",
      "\tfor a in a_tags:\n",
      "\t\ttry:\n",
      "\t\t\tlink = a['href'] #get the url from the link\n",
      "\t\t\tlink=Relative_URL_Checker(link,url) #ensure links are not relative\n",
      "\t\t\tif link not in all_links:  #make sure it's not already crawled\n",
      "\t\t\t\tall_links.append(link) #keep track of links so not crawled twice\n",
      "\t\t\t\tif \"ischool.berkeley.edu\" in link: #only crawl on ischool page\n",
      "\t\t\t\t\tlink_queue.append(link) #queue that link to be crawled\n",
      "\t\texcept: pass\n",
      "\n",
      "def SetRobotsChecker(robot_url):\n",
      "    rp = urllib.robotparser.RobotFileParser()\n",
      "    rp.set_url(robot_url)\n",
      "    rp.read()\n",
      "    return rp # returns True if it is ok to fetch this url, else False\n",
      "\n",
      "rp=SetRobotsChecker(\"http://ischool.berkeley.edu/robots.txt\") #Setup Robot Checker\n",
      "\n",
      "def OKToCrawl(rp, url):\n",
      "    return rp.can_fetch(\"*\", url)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def crawl(count=0):\n",
      "\tprint(\"\\n\\nNow crawling: \"+link_queue[0])\n",
      "\tif OKToCrawl(rp,link_queue[0]) is True:\n",
      "\t\tGetLinksFromPage(link_queue[0],count)\n",
      "\t\tdel link_queue[0]\n",
      "\t\tprint(\"total links: \"+str(len(all_links)))\n",
      "\t\tprint(\"queue length: \"+str(len(link_queue)))\n",
      "\tsleep(1) #be polite and wait 1 second\n",
      "\tcount+=1\n",
      "\tif count<40:\n",
      "\t\tcrawl(count)\n",
      "\tsaveData(webPageDataDict,'webPageDataDictFile.txt') #after crawling save the results\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'urllib' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-12-51d169c249d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrp\u001b[0m \u001b[0;31m# returns True if it is ok to fetch this url, else False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mrp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSetRobotsChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://ischool.berkeley.edu/robots.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Setup Robot Checker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mOKToCrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-12-51d169c249d0>\u001b[0m in \u001b[0;36mSetRobotsChecker\u001b[0;34m(robot_url)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mSetRobotsChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobot_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobotparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRobotFileParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobot_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'urllib' is not defined"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "inverseIndex={}\n",
      "\n",
      "def saveData(dataObj,name):\n",
      "\tsaveFile=open(name,'w')\n",
      "\tsaveFile.write(dataObj)\n",
      "\tsaveFile.close()\n",
      "\n",
      "def loadData(name):\n",
      "\tloadFile=open(name,'r')\n",
      "\tdataObj=literal_eval(loadFile.read())\n",
      "\tloadFile.close()\n",
      "\treturn dataObj\n",
      "\n",
      "\n",
      "def makeInverseIndex():\n",
      "\n",
      "\tfor webpage in webPageDataDict:\n",
      "\t\tregexPattern=re.compile('(/block-inner, /block)|\\W') #ensure only words are kept\n",
      "\t\tpageText=str(webPageDataDict[webpage][\"Text\"]).lower()\n",
      "\t\tpageText=re.sub(regexPattern,\" \",pageText)\n",
      "\t\tfor word in pageText.split():\n",
      "\t\t\tif word not in inverseIndex.keys():\n",
      "\t\t\t\tinverseIndex[word]={webPageDataDict[webpage][\"ID\"]:1} #add word to index for that webpage ID with count of 1\n",
      "\t\t\telse: #if the word is already in the index\n",
      "\t\t\t\tif webPageDataDict[webpage][\"ID\"] not in inverseIndex[word].keys(): #word in index but not the page\n",
      "\t\t\t\t\tinverseIndex[word][webPageDataDict[webpage][\"ID\"]]=1\n",
      "\t\t\t\telse: #word in index and on page already so add 1 to the count\n",
      "\t\t\t\t\tinverseIndex[word][webPageDataDict[webpage][\"ID\"]]+=1\n",
      "\tprint(inverseIndex[\"special\"])\n",
      "\t\t\n",
      "\n",
      "def main():\n",
      "\twods=scrape(\"http://berkeley-crossfit.com/2013/12/03/wod-12-03-2013/\")\n",
      "\tprint(wods)\n",
      "\n",
      "\n",
      "def main_from_homework():\n",
      "\t### Question #1\n",
      "\tcrawl() #remove comment to have script crawl the ischool.berkeley.edu site \n",
      "\t\n",
      "\t### Question #2\n",
      "\tglobal webPageDataDict   \n",
      "\twebPageDataDict=loadData('webPageDataDictFile.txt')\n",
      "\tmakeInverseIndex()\n",
      "\tsaveData(inverseIndex,\"InverseIndex.txt\") #save the Inverted Index for use later\n",
      "\n",
      "\n",
      "if __name__==\"__main__\": main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}