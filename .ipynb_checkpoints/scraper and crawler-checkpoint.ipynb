{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/local/bin/python\n",
      "# use urllib to get the header information for a web page at location url\n",
      "import urllib2\n",
      "# import urllib.parse\n",
      "# import urllib.robotparser\n",
      "from bs4 import BeautifulSoup\n",
      "from time import sleep\n",
      "import re\n",
      "from ast import literal_eval\n",
      "# import pickle\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def RequestResponse(url):\n",
      "    try:\n",
      "        response = urllib2.urlopen(url)\n",
      "        return response\n",
      "    except urllib.error.URLError as err:\n",
      "        print(\"Error opening url {} .\\nError is: {}\".format(url, err))\n",
      "\n",
      "\n",
      "def Relative_URL_Checker(url,originator_url):\n",
      "    if url[0] == '/':\n",
      "        parsed_url = urlparse(url)\n",
      "        origParsed = urlparse(originator_url)\n",
      "        hostname = \"http://\" + origParsed.netloc\n",
      "        new_full_url = urllib.parse.urljoin(hostname, parsed_url.path)\n",
      "        return new_full_url\n",
      "    else: \n",
      "    \treturn url\n",
      "    \t\n",
      "def visible(element):\n",
      "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
      "        return False\n",
      "    elif re.match('<!--.*-->', str(element)):\n",
      "        return False\n",
      "    return True\n",
      "\n",
      "link_queue=[\"http://ischool.berkeley.edu/\"] #initiate the list to store a queue of links for the crawl\n",
      "all_links=[\"http://ischool.berkeley.edu/\"] #keeps track of all links, so not crawled more than once\n",
      "webPageDataDict={}\n",
      "\n",
      "#extract all the links (a tags) from a web page; print first 10\n",
      "def GetLinksFromPage(url,ID):\n",
      "\tresponse = RequestResponse(url)\n",
      "\tif not response: return\n",
      "\tsoup = BeautifulSoup(response.readall())\n",
      "\ttitle=soup.head.title.get_text() #save title\n",
      "\ttexts=soup.body.findAll(text=True)#save visible webpage text\n",
      "\tpageText=\"\".join(x for x in filter(visible,texts)).replace(\"\\n\",\" \") # remove newline characters\n",
      "\twebPageDataDict[ID]={\"ID\":ID,\"Title\":title,\"URL\":url,\"Text\":pageText.encode(\"ascii\",\"ignore\")} #store page data in dictionary value\n",
      "\ta_tags = soup.find_all('a')\n",
      "\tfor a in a_tags:\n",
      "\t\ttry:\n",
      "\t\t\tlink = a['href'] #get the url from the link\n",
      "\t\t\tlink=Relative_URL_Checker(link,url) #ensure links are not relative\n",
      "\t\t\tif link not in all_links:  #make sure it's not already crawled\n",
      "\t\t\t\tall_links.append(link) #keep track of links so not crawled twice\n",
      "\t\t\t\tif \"ischool.berkeley.edu\" in link: #only crawl on ischool page\n",
      "\t\t\t\t\tlink_queue.append(link) #queue that link to be crawled\n",
      "\t\texcept: pass\n",
      "\n",
      "def SetRobotsChecker(robot_url):\n",
      "    rp = urllib.robotparser.RobotFileParser()\n",
      "    rp.set_url(robot_url)\n",
      "    rp.read()\n",
      "    return rp # returns True if it is ok to fetch this url, else False\n",
      "\n",
      "rp=SetRobotsChecker(\"http://ischool.berkeley.edu/robots.txt\") #Setup Robot Checker\n",
      "\n",
      "def OKToCrawl(rp, url):\n",
      "    return rp.can_fetch(\"*\", url)\n",
      "\n",
      "\n",
      "def scrape(url,DOMclass=\"entry-content\"):\n",
      "\tprint(\"scraping\")\n",
      "\tresponse = RequestResponse(url)\n",
      "\tif not response: return\n",
      "\tsoup = BeautifulSoup(response.readall())\n",
      "\t# title=soup.head.title.get_text() #save title\n",
      "\t# texts=soup.body.findAll(text=True)#save visible webpage text\n",
      "\twods=soup.find_all('div', class_=DOMclass)\n",
      "\treturn wods\n",
      "\n",
      "\n",
      "def crawl(count=0):\n",
      "\tprint(\"\\n\\nNow crawling: \"+link_queue[0])\n",
      "\tif OKToCrawl(rp,link_queue[0]) is True:\n",
      "\t\tGetLinksFromPage(link_queue[0],count)\n",
      "\t\tdel link_queue[0]\n",
      "\t\tprint(\"total links: \"+str(len(all_links)))\n",
      "\t\tprint(\"queue length: \"+str(len(link_queue)))\n",
      "\tsleep(1) #be polite and wait 1 second\n",
      "\tcount+=1\n",
      "\tif count<40:\n",
      "\t\tcrawl(count)\n",
      "\tsaveData(webPageDataDict,'webPageDataDictFile.txt') #after crawling save the results\n",
      "\n",
      "inverseIndex={}\n",
      "\n",
      "def saveData(dataObj,name):\n",
      "\tsaveFile=open(name,'w')\n",
      "\tsaveFile.write(dataObj)\n",
      "\tsaveFile.close()\n",
      "\n",
      "def loadData(name):\n",
      "\tloadFile=open(name,'r')\n",
      "\tdataObj=literal_eval(loadFile.read())\n",
      "\tloadFile.close()\n",
      "\treturn dataObj\n",
      "\n",
      "\n",
      "def makeInverseIndex():\n",
      "\n",
      "\tfor webpage in webPageDataDict:\n",
      "\t\tregexPattern=re.compile('(/block-inner, /block)|\\W') #ensure only words are kept\n",
      "\t\tpageText=str(webPageDataDict[webpage][\"Text\"]).lower()\n",
      "\t\tpageText=re.sub(regexPattern,\" \",pageText)\n",
      "\t\tfor word in pageText.split():\n",
      "\t\t\tif word not in inverseIndex.keys():\n",
      "\t\t\t\tinverseIndex[word]={webPageDataDict[webpage][\"ID\"]:1} #add word to index for that webpage ID with count of 1\n",
      "\t\t\telse: #if the word is already in the index\n",
      "\t\t\t\tif webPageDataDict[webpage][\"ID\"] not in inverseIndex[word].keys(): #word in index but not the page\n",
      "\t\t\t\t\tinverseIndex[word][webPageDataDict[webpage][\"ID\"]]=1\n",
      "\t\t\t\telse: #word in index and on page already so add 1 to the count\n",
      "\t\t\t\t\tinverseIndex[word][webPageDataDict[webpage][\"ID\"]]+=1\n",
      "\tprint(inverseIndex[\"special\"])\n",
      "\t\t\n",
      "\n",
      "def main():\n",
      "\twods=scrape(\"http://berkeley-crossfit.com/2013/12/03/wod-12-03-2013/\")\n",
      "\tprint(wods)\n",
      "\n",
      "\n",
      "def main_from_homework():\n",
      "\t### Question #1\n",
      "\tcrawl() #remove comment to have script crawl the ischool.berkeley.edu site \n",
      "\t\n",
      "\t### Question #2\n",
      "\tglobal webPageDataDict   \n",
      "\twebPageDataDict=loadData('webPageDataDictFile.txt')\n",
      "\tmakeInverseIndex()\n",
      "\tsaveData(inverseIndex,\"InverseIndex.txt\") #save the Inverted Index for use later\n",
      "\n",
      "\n",
      "if __name__==\"__main__\": main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}